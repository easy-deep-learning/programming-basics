# Введение. Почему двоичное число?

Двоичное исчисление, двоичная математика — фундамент компьютерных вычислений.
Хотя набор нулей и единиц выглядит загадочным шифром — это ключ, открывающий двери в понимание, как работает компьютер.
"Структруры данных", "Алгоритмы", "Паттерны" и другие слова, которые ты много раз видишь с книжках и статьях по программированию — с ощущением, как это работает внутри железа — даст ясную картину мира программирования.

Когда открываешь учебник по двоичному исчислению, обычно быстро начинает кружиться голова от нулей, единиц, непривычного способа сложения-умножения. Но создатели первых компьюторов разумеется, не хотели нас помучать.
Просто, двоичная математика — проще... для реализации в электронной схеме.

Так как электронная схема (компьютер в данной главе) ничего о нашем реальном мире не знает, нужно как-то перевести нашу задачу на ее язык. Электронная схема, судя по названию работает с электронами. Точнее с потоками электронов. Примерно, как работает выключатель и люстра в вашей комнате. Включил — есть свет, выключил — света нет. В особо продвинутых лампах есть регулировка яркости.
Так вот, поначалу разработчики хотели использовать привычную систему — десятичную. Так сложилось, что у нас 10 пальцев и поэтому счет изобрели с помощью 10 цифр.
Как представить 10 цифр в электронной схеме? Допустим нам повезло, и наша лампа имеет регулировку яркости. Представим каждую цифру как интенсивность света, 9 самый яркий и до нуля:

<table>
   <tr>
      <td bgcolor="#E8E8E8">9</td>
      <td style="background:#E8E8E8"></td>
      <td style="background:#D3D3D3></td>
   </tr>
</table>
   
